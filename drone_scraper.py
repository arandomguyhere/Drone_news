#!/usr/bin/env python3
"""
Drone Intelligence Scraper for Drone_news Repository
Runs entirely on GitHub Actions - No local setup required
"""

import urllib.request
import urllib.parse
import json
import os
import time
import random
import sys
from datetime import datetime

print("ğŸš DRONE INTELLIGENCE COLLECTION SYSTEM")
print("ğŸ“Š Repository: Drone_news")
print("ğŸŒ Running on GitHub Actions")
print("=" * 60)

def create_directories():
    """Create required directories"""
    os.makedirs("data", exist_ok=True)
    os.makedirs("docs", exist_ok=True)
    print("âœ… Directories created")

def get_search_queries():
    """Get comprehensive drone intelligence search queries"""
    
    queries = [
        ("drone when:24h", "ğŸš Drones"),
        ("UAV when:24h", "ğŸ›©ï¸ UAV"),
        ("UAS when:24h", "ğŸ›©ï¸ UAS"),
        ("military drone when:24h", "ğŸ¯ Military Drones"),
        ("China drone when:24h", "ğŸ‡¨ğŸ‡³ China Drones"),
        ("Russia drone when:24h", "ğŸ‡·ğŸ‡º Russia Drones"),
        ("autonomous drone when:24h", "ğŸ¤– Autonomous Drones"),
        ("drone warfare when:24h", "âš”ï¸ Drone Warfare"),
        ("Iran drone when:24h", "ğŸ‡®ğŸ‡· Iran Drones"),
        ("drone strike when:24h", "ğŸ’¥ Drone Strikes"),
        ("anti-drone when:24h", "ğŸ›¡ï¸ Counter-Drone"),
        ("drone swarm when:24h", "ğŸ Drone Swarms"),
        ("combat drone when:24h", "âš”ï¸ Combat Systems"),
        ("quadcopter when:24h", "ğŸš Quadcopters"),
        ("tactical UAV when:24h", "ğŸ¯ Tactical UAV"),
        ("North Korea drone when:24h", "ğŸ‡°ğŸ‡µ DPRK Drones"),
        ("Ukraine drone when:24h", "ğŸ‡ºğŸ‡¦ Ukraine Drones"),
        ("Israel drone when:24h", "ğŸ‡®ğŸ‡± Israel Drones"),
        ("Turkey drone when:24h", "ğŸ‡¹ğŸ‡· Turkey Drones"),
        ("FPV drone when:24h", "ğŸ® FPV Systems"),
        ("VTOL drone when:24h", "ğŸš VTOL Systems"),
        ("commercial drone when:24h", "ğŸ“¦ Commercial Drones"),
        ("drone delivery when:24h", "ğŸ“¦ Delivery Services"),
        ("agricultural drone when:24h", "ğŸšœ Agricultural Drones"),
        ("drone regulation when:24h", "ğŸ“‹ Regulation"),
        ("FAA drone when:24h", "ğŸ“‹ FAA Policy"),
        ("site:reuters.com drone when:24h", "ğŸ“º Reuters"),
        ("site:defensenews.com drone when:24h", "ğŸ“° Defense News"),
        ("site:janes.com drone when:24h", "ğŸ“° Jane's Defence"),
        ("site:cnn.com drone when:24h", "ğŸ“º CNN"),
        ("site:bbc.com drone when:24h", "ğŸ“º BBC"),
        ("site:bloomberg.com drone when:24h", "ğŸ“º Bloomberg"),
        ("site:wsj.com drone when:24h", "ğŸ“º Wall Street Journal"),
        ("site:thedrive.com drone when:24h", "ğŸ“° The Drive"),
        ("site:wired.com drone when:24h", "ğŸ’» Wired")
    ]
    
    # Use priority mode if specified
    if '--priority' in sys.argv:
        print("ğŸš€ PRIORITY MODE: Using 15 high-impact searches")
        return queries[:15]
    
    print(f"ğŸ” COMPREHENSIVE MODE: Using {len(queries)} searches")
    return queries

def search_google_news(query, category):
    """Search Google News for drone intelligence"""
    
    print(f"  ğŸ” Searching: {category}")
    
    try:
        # Encode query for URL
        encoded_query = urllib.parse.quote(query.encode('utf-8'))
        url = f'https://news.google.com/search?q={encoded_query}&hl=en'
        
        # Add respectful delay
        time.sleep(random.uniform(1, 3))
        
        # Make request with proper headers
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        req = urllib.request.Request(url, headers=headers)
        
        with urllib.request.urlopen(req, timeout=15) as response:
            content = response.read().decode('utf-8', errors='ignore')
        
        # Extract article information using pattern matching
        articles = []
        
        # Import regex for pattern matching
        import re
        
        # Look for article-like patterns in the HTML
        patterns = [
            r'"([^"]{30,150})"',  # Quoted text (likely titles)
            r'aria-label="([^"]{30,150})"',  # Aria labels
            r'<h[1-6][^>]*>([^<]{30,150})</h[1-6]>'  # Header tags
        ]
        
        found_titles = set()
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                title = match.strip()
                
                # Filter for drone-related content
                drone_keywords = ['drone', 'uav', 'uas', 'unmanned', 'aircraft', 'quadcopter']
                if (len(title) >= 30 and len(title) <= 150 and
                    title not in found_titles and
                    any(keyword in title.lower() for keyword in drone_keywords) and
                    not any(skip in title.lower() for skip in ['cookie', 'privacy', 'terms', 'subscribe', 'newsletter', 'advertisement'])):
                    
                    found_titles.add(title)
                    articles.append({
                        'Title': title,
                        'Link': url,
                        'Source': f"{category.split()[-1]} Source",
                        'Published': 'Recent',
                        'Category': category,
                        'Image': None,
                        'Scraped_At': datetime.now().isoformat()
                    })
                    
                    if len(articles) >= 6:  # Limit per search
                        break
            
            if len(articles) >= 6:
                break
        
        print(f"    âœ… Found {len(articles)} articles")
        return articles
        
    except Exception as e:
        print(f"    âŒ Error: {str(e)[:50]}...")
        return []

def collect_all_intelligence():
    """Main intelligence collection function"""
    
    queries = get_search_queries()
    all_articles = []
    
    print(f"ğŸ“Š Starting collection from {len(queries)} sources...")
    
    for i, (query, category) in enumerate(queries):
        try:
            articles = search_google_news(query, category)
            all_articles.extend(articles)
            
            # Progress update every 5 searches
            if (i + 1) % 5 == 0:
                print(f"ğŸ“ˆ Progress: {i+1}/{len(queries)} searches completed ({len(all_articles)} articles so far)")
                
        except Exception as e:
            print(f"âŒ Error in search {i+1}: {e}")
            continue
    
    # Remove duplicates based on title similarity
    unique_articles = []
    seen_titles = set()
    
    for article in all_articles:
        title_key = article['Title'].lower().strip()
        
        # Simple word-based deduplication
        title_words = set(title_key.split())
        is_duplicate = False
        
        for seen_title in seen_titles:
            seen_words = set(seen_title.split())
            if len(title_words) > 0 and len(seen_words) > 0:
                # Check for 70%+ word overlap
                overlap = len(title_words.intersection(seen_words))
                similarity = overlap / max(len(title_words), len(seen_words))
                if similarity > 0.7:
                    is_duplicate = True
                    break
        
        if not is_duplicate:
            seen_titles.add(title_key)
            unique_articles.append(article)
    
    print(f"âœ… Collection completed!")
    print(f"ğŸ“Š Total articles found: {len(all_articles)}")
    print(f"ğŸ“Š Unique articles: {len(unique_articles)}")
    print(f"ğŸ“Š Duplicate removal: {len(all_articles) - len(unique_articles)} duplicates filtered")
    
    return unique_articles

def save_intelligence_data(articles):
    """Save intelligence data to JSON and CSV files"""
    
    try:
        # Always save JSON file (even if empty for GitHub Actions)
        with open("data/latest_news.json", "w", encoding="utf-8") as f:
            json.dump(articles, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Saved {len(articles)} articles to data/latest_news.json")
        
        # Try to save CSV if pandas is available
        try:
            import pandas as pd
            if articles:
                df = pd.DataFrame(articles)
                df.to_csv("data/latest_news.csv", index=False)
                print(f"âœ… Saved CSV to data/latest_news.csv")
            else:
                # Create empty CSV
                with open("data/latest_news.csv", "w") as f:
                    f.write("Title,Link,Source,Published,Category,Image,Scraped_At\n")
                print(f"âœ… Created empty CSV file")
        except ImportError:
            print("âš ï¸ Pandas not available for CSV export")
        except Exception as e:
            print(f"âš ï¸ CSV export failed: {e}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Save error: {e}")
        return False

def show_collection_summary(articles):
    """Display comprehensive collection summary"""
    
    print(f"\nğŸ“Š COLLECTION SUMMARY")
    print(f"=" * 50)
    
    if not articles:
        print("ğŸ“Š No articles collected this session")
        print("ğŸ”„ System will retry on next scheduled run")
        return
    
    # Calculate statistics
    categories = {}
    sources = set()
    recent_articles = 0
    
    for article in articles:
        cat = article.get('Category', 'Unknown')
        src = article.get('Source', 'Unknown')
        pub = article.get('Published', '').lower()
        
        categories[cat] = categories.get(cat, 0) + 1
        sources.add(src)
        
        if any(term in pub for term in ['hour', 'minute', 'recent']):
            recent_articles += 1
    
    print(f"ğŸ“ˆ Total Intelligence Reports: {len(articles)}")
    print(f"ğŸ“‚ Categories Covered: {len(categories)}")
    print(f"ğŸ“° Unique Sources: {len(sources)}")
    print(f"ğŸ• Recent Reports (hours): {recent_articles}")
    
    # Show top categories
    print(f"\nğŸ† Top Intelligence Categories:")
    sorted_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)
    for i, (cat, count) in enumerate(sorted_categories[:10]):
        print(f"  {i+1:2d}. {cat}: {count} reports")
    
    # Show collection rate
    print(f"\nâš¡ Collection Efficiency:")
    total_searches = len(get_search_queries())
    success_rate = (len([cat for cat, count in categories.items() if count > 0]) / total_searches) * 100
    print(f"  ğŸ“Š Search Success Rate: {success_rate:.1f}%")
    print(f"  ğŸ“ˆ Articles per Search: {len(articles) / total_searches:.1f}")

def main():
    """Main execution function optimized for GitHub Actions"""
    
    try:
        # Setup environment
        create_directories()
        
        # Collect intelligence
        print(f"\nğŸ¯ STARTING INTELLIGENCE COLLECTION")
        print(f"ğŸ• Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}")
        
        start_time = time.time()
        articles = collect_all_intelligence()
        collection_time = time.time() - start_time
        
        # Save data (always save, even if empty)
        if save_intelligence_data(articles):
            show_collection_summary(articles)
            
            print(f"\nğŸ‰ INTELLIGENCE COLLECTION SUCCESS!")
            print(f"â±ï¸  Collection Time: {collection_time:.1f} seconds")
            print(f"ğŸ“ Data saved for GitHub Pages deployment")
            print(f"ğŸŒ Live Brief: https://{os.environ.get('GITHUB_REPOSITORY_OWNER', 'your-username')}.github.io/Drone_news/")
            
        else:
            print(f"âŒ Failed to save intelligence data")
            
    except Exception as e:
        print(f"âŒ System error: {e}")
        import traceback
        traceback.print_exc()
        
        # Always create empty data file for GitHub Actions
        try:
            with open("data/latest_news.json", "w") as f:
                json.dump([], f)
            print("ğŸ“ Created empty data file for GitHub Actions continuity")
        except:
            pass

if __name__ == "__main__":
    main()
